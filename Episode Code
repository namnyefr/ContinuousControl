def dqn(n_episodes=5, max_t=500):
    """Deep Q-Learning.

    Params
    ======
        n_episodes (int): maximum number of training episodes
        max_t (int): maximum number of timesteps per episode
    """
    from collections import deque
    import torch 
    window = 1

    agent = Agent(state_size, action_size, 0)

    scores = []                        # list containing scores from each episode
    #print("Initial values of scores", scores)
    scores_window = deque(maxlen=window)  # last 10 scores
    #print("Last 100 scores", scores_window)
    #env_info = env.reset(train_mode=True)[brain_name]

    for i_episode in range(1, n_episodes+1):
        #agent = Agent(state_size, action_size, seed=0)
        #agent = Agent(state_size, action_size, 1234)
        env_info = env.reset(train_mode=True)[brain_name]
        agent.reset()
        #print("Initial value of reward from environment",env_info.rewards[0])
        #state = env.reset()
        states = env_info.vector_observations[0]
        #print("Initial State is", state)
        score = 0
        t=0
        for t in range(max_t):
        #while True:
            #t += 1
            #actions = np.random.randn(num_agents, action_size) # select an action (for each agent)
            #actions = np.clip(actions, -1, 1)
            actions=agent.act(states)
            env_info = env.step(actions)[brain_name]        # send the action to the environment
            #Provides the environment with an action, moves the environment dynamics forward accordingly, and returns
            #observation, state, and reward information to the agent.
            #:param vector_action: Agent's vector action to send to environment. Can be a scalar or vector of int/floats.
            #:param memory: Vector corresponding to memory used for RNNs, frame-stacking, or other auto-regressive process.
            #:param text_action: Text action to send to environment for.
            #:return: AllBrainInfo  : A Data structure corresponding to the new state of the environment.
            next_states = env_info.vector_observations[0]   # get the next state
            rewards = env_info.rewards[0]                   # get the reward
            dones = env_info.local_done[0]
            #print("Current State at time, ", t," is,",states)
            #print("Action corresponding to Current State is, ", actions)
            #print("Reward for Next State is ,", rewards)
            #print("Next State is,",next_states)
            #print('Is it done for time,', t,'?', dones)
            agent.step(states, actions, rewards, next_states, dones)
            #step function includes sampling from previous tuples (states, actions, rewards, next_states, 
            #dones) are defined by the tuple named 'experiences' defined by the sample function of the memory
            #function that equates to the ReplyBuffer in the Agent init function)
            #step also adds the resultant info to the replay buffer for later sampling
            states = next_states
            score += rewards
            if rewards != 0:
                print("Current reward is, ", rewards, "For a total reward of,", score)
            #if rewards != 0.0:
                #print("Reward", rewards, "at step", t, "for a total score of ", score)
            #if done:
            if np.any(dones):             # exit loop if episode finished
                #print("Final value of reward received in episode", i_episode, " is ", rewards)
                break 
        scores_window.append(score)       # save most recent score
        #print("Scores window", scores_window)
        scores.append(score)              # save most recent score
        #print("Episode", i_episode, "Scores", scores)
        #eps = max(eps_end, eps_decay*eps) # decrease epsilon
        if i_episode % window == 0: 
            print('\rEpisode {}\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end="")
            with open('avg_scores36.txt', 'a') as f:
                f.write("%s %s\n" % (str(i_episode),str(np.mean(scores_window))))
        #if i_episode % 1 == 0:
            print('\rEpisode {}\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))
        #if np.mean(scores_window)>=200.0:
        if np.mean(scores_window)>=30.0:
            print('\nEnvironment solved in {:d} episodes!\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))
            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint4.pth')
            break
    return scores

scores = dqn()
#print("Scores array", scores)

# plot the scores
fig = plt.figure()
ax = fig.add_subplot(111)
print("Episode max", len(scores))
plt.plot(np.arange(len(scores)), scores)
plt.ylabel('Score')
plt.xlabel('Episode #')
plt.show()
